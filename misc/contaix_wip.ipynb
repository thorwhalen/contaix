{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tools for AI contexts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import contaix as contexts  # extract_urls, notebook_to_markdown, aggregate_store, code_aggregate, verify_urls, etc.\n",
                "from oa.chats import ChatDacc  # currently broken\n",
                "from scraped import markdown_of_site, download_site, scrape_multiple_sites\n",
                "import hubcap\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## github discussions (etc.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from hubcap import RepoReader\n",
                "\n",
                "url = 'https://github.com/thorwhalen/sonification'\n",
                "r = RepoReader(url)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[4, 1]"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "discussions = r['discussions']\n",
                "list(discussions)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# jdict = dict(discussions)  # get all discussions as a dict\n",
                "jdict = discussions[4]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['number', 'title', 'body', 'author', 'createdAt', 'updatedAt', 'comments']"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "list(jdict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'body': '## Python Libraries for Sonification and Music Synthesis\\r\\n\\r\\nWe‚Äôll look into Python libraries and tools that can help transform numerical feature vectors representing sentiments into musical or auditory forms. This includes tools that support MIDI generation, audio synthesis, real-time playback, and symbolic music generation, while enabling both symbolic and acoustic mappings. We‚Äôll focus on options that support polyphonic outputs and span from high-level abstractions to low-level sound design libraries\\r\\n\\r\\nThis list includes Python tools for converting numerical feature vectors (e.g., emotion scores) into sound or music using both symbolic (MIDI, notes, instruments) and acoustic (pitch, volume, timbre) mappings. All tools support polyphony and span a range of abstraction levels.\\r\\n\\r\\n---\\r\\n\\r\\n### üéµ High-Level Sonification Frameworks\\r\\n\\r\\n- **[Astronify](https://github.com/spacetelescope/astronify)**  \\r\\n  Time-series data to musical sound. Originally for astronomy but works with any 1D data. Maps values to pitch, time, and gain.\\r\\n\\r\\n- **[Sci-Sonify](https://github.com/mrahim/sci-sonify)**  \\r\\n  Adds `.sonify()` methods to pandas, NumPy, xarray. Uses ‚Äúsoundmaps‚Äù to map values to musical note ranges, scales automatically.\\r\\n\\r\\n- **[Miditime](https://github.com/therevoman/miditime)**  \\r\\n  Converts time-series data to MIDI. Maps data values to pitch, duration, and velocity. Outputs standard MIDI files.\\r\\n\\r\\n- **[audio-plot-lib](https://github.com/FlorianWilhelm/audio-plot-lib)**  \\r\\n  Sonifies matplotlib-style plots. Data points map to pitches, x-axis to time/panning. Great for making trends audible.\\r\\n\\r\\n- **[SonoUno](https://gitlab.com/fi-uba-sonouno/sonouno)**  \\r\\n  GUI and Python API for turning data into sound. Supports real-time playback and audio file export, with preprocessing tools.\\r\\n\\r\\n---\\r\\n\\r\\n### üéº Symbolic Music & MIDI Tools\\r\\n\\r\\n- **[music21](https://web.mit.edu/music21/)**  \\r\\n  Music theory + composition toolkit. Create notes, chords, and streams. Export to MIDI/MusicXML. High-level symbolic mapping.\\r\\n\\r\\n- **[Mingus](https://bspaans.github.io/python-mingus/)**  \\r\\n  Advanced music theory in Python. Includes MIDI sequencing + FluidSynth support for real-time playback with instruments.\\r\\n\\r\\n- **[SCAMP](https://scamp.marcevanstein.com/)**  \\r\\n  Suite for computer-assisted music in Python. Real-time playback, multi-instrument scheduling, MIDI/audio export.\\r\\n\\r\\n- **[MIDIUtil](https://github.com/MarkCWirt/MIDIUtil)**  \\r\\n  Simple MIDI file generation from Python. Add notes with timing, pitch, duration, and velocity.\\r\\n\\r\\n- **[PrettyMIDI](https://github.com/craffel/pretty-midi)**  \\r\\n  Powerful MIDI structure manipulation. Can synthesize basic audio. Supports tempo, velocity, instrument handling.\\r\\n\\r\\n- **[Partitura](https://github.com/musescore/partitura)**  \\r\\n  MusicXML/MIDI parsing and generation. More for score analysis but usable for symbolic sonification.\\r\\n\\r\\n---\\r\\n\\r\\n### üéõÔ∏è Audio Synthesis & DSP Libraries\\r\\n\\r\\n- **[Pyo](https://ajaxsoundstudio.com/software/pyo/)**  \\r\\n  Real-time audio synthesis and DSP. Create oscillators, filters, envelopes. Polyphonic and highly customizable.\\r\\n\\r\\n- **[AudioLazy](https://pypi.org/project/audiolazy/)**  \\r\\n  Pure Python DSP toolkit. Streams, filters, synthesizers. Maps data to audio signals and effects.\\r\\n\\r\\n- **[python-musical](https://github.com/lechmazur/python-musical)**  \\r\\n  Music theory + waveform generators. Create notes/chords and directly generate audio from them.\\r\\n\\r\\n- **[ctcsound (Csound Python API)](https://csound.github.io/docs/api/python/)**  \\r\\n  Bindings for the Csound audio engine. Allows full control over synthesis via score + instrument definitions.\\r\\n\\r\\n- **[FoxDot](https://foxdot.org/)**  \\r\\n  High-level live coding environment in Python for SuperCollider. Real-time pattern/music generation.\\r\\n\\r\\n- **[PyFluidSynth](https://pypi.org/project/PyFluidSynth/)**  \\r\\n  Python interface to FluidSynth for real-time MIDI playback using SoundFonts.\\r\\n\\r\\n- **[PyDub](https://github.com/jiaaro/pydub)**  \\r\\n  Audio editing and simple tone generation. Combine tones, layer segments, export audio.\\r\\n\\r\\n- **[PySynth](https://github.com/mdoege/PySynth)**  \\r\\n  Lightweight synthesizer that outputs WAV from note lists. Basic, but easy for quick prototyping.\\r\\n\\r\\n---\\r\\n\\r\\nEach of these tools can be integrated into a Python pipeline to sonify data based on mappings you define. For richer acoustic textures, prefer `Pyo`, `SCAMP`, or `FoxDot`. For simpler symbolic mappings, use `music21`, `Mingus`, or `Miditime`.',\n",
                            " 'author': 'thorwhalen',\n",
                            " 'replies': []}"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "subdict = jdict['comments'][-1]\n",
                "subdict"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "76326"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from hubcap import create_markdown_from_discussion_jdict\n",
                "\n",
                "# TODO: Make it work for subdict of discussion jdic\n",
                "# discussion_md = create_markdown_from_discussion_jdict(subdict)\n",
                "\n",
                "discussion_md = create_markdown_from_discussion_jdict(jdict)\n",
                "len(discussion_md)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# copy the markdown (to paste it elsewhere)\n",
                "from pyperclip import copy\n",
                "copy(discussion_md)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ... or save it to a file\n",
                "# import pathlib \n",
                "# pathlib.Path('sonification_discussion.md').write_text(discussion_md)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extract urls from markdown"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "## Python Libraries for Sonification and Music Synthesis\n",
                        "\n",
                        "We‚Äôll look into Python libraries and tools that can help transform numerical feature vectors representing sentiments into musical or auditory forms. This includes tools that support MIDI generation, audio synthesis, real-time playback, and symbolic music generation, while enabling both symbolic and acoustic mappings. We‚Äôll focus on options that support polyphonic outputs and span from high-level abstractions to low-level sound design libraries\n",
                        "\n",
                        "This list includes Python tools for converting numerical feature vectors (e.g., emotion scores) into sound or music using both symbolic (MIDI, notes, instruments) and acoustic (pitch, volume, timbre) mappings. All tools support polyphony and span a range of abstraction levels.\n",
                        "\n",
                        "---\n",
                        "\n",
                        "### üéµ High-Level Sonification Frameworks\n",
                        "\n",
                        "- **[Astronify](https://github.com/spacetelescope/astronify)**  \n",
                        "  Time-series data to musical sound. Originally for astronomy but works with any 1D data. Maps values to pitch, time, and gain.\n",
                        "\n",
                        "- **[Sci-Sonify](https://github.com/mrahim/sci-sonify)**  \n",
                        "  Adds `.sonify()` methods to pandas, NumPy, xarray. Uses ‚Äúsoundmaps‚Äù to map values to musical note ranges, scales automatically.\n",
                        "\n",
                        "- **[Miditime](https://github.com/therevoman/miditime)**  \n",
                        "  Converts time-series data to MIDI. Maps data values to pitch, duration, and velocity. Outputs standard MIDI files.\n",
                        "\n",
                        "- **[audio-plot-lib](https://github.com/FlorianWilhelm/audio-plot-lib)**  \n",
                        "  Sonifies  ...\n",
                        "...\n"
                    ]
                }
            ],
            "source": [
                "content = jdict['comments'][-1]['body']\n",
                "print(content[:1500], '...\\n...')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['html_links', 'only_urls', 'with_surrounding_context']"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from contaix import extract_urls\n",
                "\n",
                "[x for x in dir(extract_urls) if not x.startswith('_')]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'Astronify': 'https://github.com/spacetelescope/astronify',\n",
                            " 'Sci-Sonify': 'https://github.com/mrahim/sci-sonify',\n",
                            " 'Miditime': 'https://github.com/therevoman/miditime',\n",
                            " 'audio-plot-lib': 'https://github.com/FlorianWilhelm/audio-plot-lib',\n",
                            " 'SonoUno': 'https://gitlab.com/fi-uba-sonouno/sonouno',\n",
                            " 'music21': 'https://web.mit.edu/music21/',\n",
                            " 'Mingus': 'https://bspaans.github.io/python-mingus/',\n",
                            " 'SCAMP': 'https://scamp.marcevanstein.com/',\n",
                            " 'MIDIUtil': 'https://github.com/MarkCWirt/MIDIUtil',\n",
                            " 'PrettyMIDI': 'https://github.com/craffel/pretty-midi',\n",
                            " 'Partitura': 'https://github.com/musescore/partitura',\n",
                            " 'Pyo': 'https://ajaxsoundstudio.com/software/pyo/',\n",
                            " 'AudioLazy': 'https://pypi.org/project/audiolazy/',\n",
                            " 'python-musical': 'https://github.com/lechmazur/python-musical',\n",
                            " 'ctcsound (Csound Python API)': 'https://csound.github.io/docs/api/python/',\n",
                            " 'FoxDot': 'https://foxdot.org/',\n",
                            " 'PyFluidSynth': 'https://pypi.org/project/PyFluidSynth/',\n",
                            " 'PyDub': 'https://github.com/jiaaro/pydub',\n",
                            " 'PySynth': 'https://github.com/mdoege/PySynth'}"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dict(extract_urls.markdown_links(content))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Gather information from some of these urls"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "github_urls = {\n",
                "    'Astronify_github': 'https://github.com/spacetelescope/astronify',\n",
                "    'Sci-Sonify_github': 'https://github.com/philipc2/sci-sonify',  # other (AI generated) repo didn't exist\n",
                "    'Miditime_github': 'https://github.com/mikejcorey/miditime',  # other (AI generated) repo didn't exist\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<function hubcap.repo_slurp.repo_text_aggregate(repo, kinds: Union[Literal['files', 'wiki', 'discussions'], Iterable[Literal['files', 'wiki', 'discussions']]] = ('files', 'wiki', 'discussions'), *, github_repo_mapping=<function github_repo_mapping at 0x17c1ffc70>, text_from_mapping=<function text_from_mapping at 0x17c1ff250>)>"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from hubcap import repo_text_aggregate\n",
                "\n",
                "repo_text_aggregate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Miditime_github: https://github.com/mikejcorey/miditime\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Cloning into '/var/folders/mc/c070wfh51kxd9lft8dl74q1r0000gn/T/tmp5muoc8j9'...\n"
                    ]
                }
            ],
            "source": [
                "import pathlib \n",
                "import os \n",
                "\n",
                "rootdir = '/Users/thorwhalen/Dropbox/_odata/ai_contexts/documentation'\n",
                "\n",
                "for name, url in github_urls.items():\n",
                "    print(f\"{name}: {url}\")\n",
                "    text = repo_text_aggregate(url, kinds=['files'])\n",
                "    pathlib.Path(os.path.join(rootdir, f'{name}.md')).write_text(text)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Information from other urls"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "urls = {\n",
                "    \"Astronify_docs\": \"https://astronify.readthedocs.io/en/latest/astronify/index.html\",\n",
                "    \"Astronify_tutorials\": \"https://astronify.readthedocs.io/en/latest/astronify/tutorials.html\",\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scraped import markdown_of_site\n",
                "import os \n",
                "\n",
                "rootdir = '/Users/thorwhalen/Dropbox/_odata/ai_contexts/documentation'\n",
                "\n",
                "for name, url in urls.items():\n",
                "    markdown_of_site(url, depth=2, save_filepath=os.path.join(rootdir, f'{name}.md'))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Extracting info from notebooks\n",
                "\n",
                "Sometimes we want to get some jupyter notebook stuff into the AI context. \n",
                "Notebooks are stored as quite verbose jsons, so what I often do is to convert the notebook to markdown. \n",
                "Even then, I might get more than I want, which will go over AI context limits, or at the very least infect \n",
                "the signal-to-noise ratio. \n",
                "\n",
                "So I want to filter stuff (in and/or out). \n",
                "\n",
                "I can do that pre-conversion (on the json) or post-conversion (on the markdown). \n",
                "Both have their place. \n",
                "\n",
                "One con of doing this on the json is that it's more information rich, so can be more complicated to find \n",
                "what you need to filter in or out. \n",
                "One pro is that the json is structured, so it's easier to express many of your filtering needs\n",
                "(less need for regular expressions etc.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from contaix import notebook_to_markdown\n",
                "from test2doc.notebook_utils import clear_outputs_of_largest_output_cells, ensure_notebook_dict"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'## Astronify\\'s Spectrum preview mode with OBAFGKM samples\\nBased on data from the MILES library service developed by the Spanish Virtual Observatory in the framework of the IAU Comission G5 Working Group : Spectral Stellar Libraries\\n\\n\\n```python\\nfrom astropy.io import fits,ascii\\nimport numpy as np\\nimport os\\nimport requests\\nimport matplotlib.pyplot as plt\\n\\nfrom astropy.table import QTable, Table, Column\\nfrom astronify.series import SoniSeries\\n```\\n\\n\\n```python\\n# File names to save each file when downloaded for the Miles exmaple spectra.\\nostar_filename = \"1-Type_O_Stelib_HD269698.fits\"\\nbstar_filename = \"2-Type_B_HD003369_s0020.fits\"\\nastar_filename = \"3-Type_A_HD031295_s0166.fits\"\\nfstar_filename = \"4-Type_F_HD222451_s0889.fits\"\\ngstar_filename = \"5-Type_G_HD114606_s0462.fits\"\\nkstar_filename = \"6-Type_K_HD233832_s0410.fits\"\\nmstar_filename = \"7-Type_M_HD036395_s0183.fits\"\\n\\nall_filenames = np.asarray([ostar_filename, bstar_filename, astar_filename, fstar_filename,\\n                 gstar_filename, kstar_filename, mstar_filename])\\nn_stars = len(all_filenames)\\n```\\n\\n\\n```python\\n# These \"share\" URLs are used to download the sample files.\\nostar_link = \"https://stsci.box.com/shared/static/v7eecpzpxfnb3fxywy0amve4ofcoz0ns\"\\nbstar_link = \"https://stsci.box.com/shared/static/vpoby26z4f7cm9mavlo7fziikb1s3v9n\"\\nastar_link = \"https://stsci.box.com/shared/static/wmmwy5im68lnhjcw63iyc8rz3n65f1cr\"\\nfstar_link = \"https://stsci.box.com/shared/static/ro5ix00yh19iid9wxlzgi41bjfbremtz\"\\ngstar_link = \"https://stsci.box.com/shared/static/yv13duxb5qqtjdfgmsirh8rbyw3r68s2\"\\nkstar_link = \"https://stsci.box.com/shared/static/zbqy0bzesz7z8mqu0h0nqzffbnc4h0xg\"\\nmstar_link = \"https://stsci.box.com/shared/static/ztq2x6vsx7ickq0zmmimb7qhguu8vz3a\"\\n\\nall_urls = np.asarray([ostar_link, bstar_link, astar_link, fstar_link, gstar_link,\\n                       kstar_link, mstar_link])\\n```\\n\\n\\n```python\\n# Download each sample spectrum to the local working directory.\\nodir = \"miles_stellar_spectra\"\\nif not os.path.isdir(odir):\\n    os.makedirs(odir)\\n```\\n\\n\\n```python\\n# Download the sample spectra.\\nfor file, url in zip(all_filenames, all_urls):\\n    print(\"Downloading \" + file + \" via \" + url + \"...\")\\n    response = requests.get(url)\\n    open(odir + os.path.sep + file, \"wb\").write(response.content)\\n```\\n\\n    Downloading 1-Type_O_Stelib_HD269698.fits via https://stsci.box.com/shared/static/v7eecpzpxfnb3fxywy0amve4ofcoz0ns...\\n    Downloading 2-Type_B_HD003369_s0\\n    ...\\n    shared/static/ztq2x6vsx7ickq0zmmimb7qhguu8vz3a...\\n\\n\\n\\n```python\\n# Read in the wavelengths and fluxes of each spectrum.\\n# We\\'ll store them as a list of dict objects contiaining the wavelengths and fluxes of\\n# the seven sample spectra.\\nall_spectra = []\\nfor ii, file in enumerate(all_filenames):\\n    file = odir + os.path.sep + file\\n    if os.path.isfile(file):\\n        with fits.open(file) as hdulist:\\n            # Read in flux from the data table.\\n            flux = np.array(hdulist[0].data)\\n            # Normalize the flux by the maximum value.\\n            flux_norm = np.reshape(flux/(np.nanmax(flux)), (hdulist[0].header[\\'NAXIS1\\']))\\n            # Setup list of wavelengths.\\n            wave = np.ones(hdulist[0].header[\"NAXIS1\"], dtype=float)\\n            # Compute the wavelength values from the WCS header keywords.\\n            for i in range(hdulist[0].header[\"NAXIS1\"]):\\n                wave[i] = hdulist[0].header[\"CRVAL1\"] + i*hdulist[0].header[\"CDELT1\"]\\n            hdulist.close()\\n        # Add this star\\'s wavelength and fluxes to the dict object.\\n        this_spec = dict()\\n        this_spec[\"wls\"] = wave\\n        this_spec[\"fls\"] = flux_norm\\n        all_spectra.append(this_spec)\\n    else:\\n        raise IOError(\"Could not find expected input file: \" + file)\\n```\\n\\n\\n```python\\n# Construct the Sonification object.\\nsoni_table = Table([all_spectra[0][\"wls\"], all_spectra[0][\"fls\"]],\\n                             names=[\"wavelengths\", \"flux\"])\\n\\n# In an \"ensemble\" preview, each section is a different pitch frequency.  Each section\\n# gets played separately, then at the end all sections get played together.\\ndata_soni_ensemble = SoniSeries(soni_table, time_col=\"wavelengths\", val_col=\"flux\",\\n                                preview_type=\"ensemble\")\\nensemble_prev = data_soni_ensemble.preview_object\\nensemble_prev.sonify_preview()\\nensemble_prev.play_preview()\\n```\\n\\n    HTML output truncated. (Data removed)\\n\\n\\n    Pyo warning: Portaudio input device `MacBook Pro Microphone` has fewer channels (1) than requested (2).\\n    Pyo warning: Portmidi warning: no midi device found!\\n    Portmidi closed.\\n\\n\\n\\n```python\\n# In a \"scan\" preview, each section has the same frequency.  Each section gets\\n# played separately, and then there is no combined sound made.\\ndata_soni_scan = SoniSeries(soni_table, time_col=\"wavelengths\", val_col=\"flux\",\\n                       preview_type=\"scan\")\\nscan_prev = data_soni_scan.preview_object\\nscan_prev.sonify_preview()\\nscan_prev.play_preview()\\n```\\n\\n    HTML output truncated. (Data removed)\\n\\n\\n    Pyo warning: Portaudio input device `MacBook Pro Microphone` has fewer channels (1) than requested (2).\\n    Pyo warning: Portmidi warning: no midi device found!\\n    Portmidi closed.\\n\\n\\n\\n```python\\ndata_soni = SoniSeries(spectrum_B)\\ndata_soni_preview = data_soni.preview_object\\ndata_soni_preview.sonify_preview()\\ndata_soni_preview.play_preview()\\n```\\n\\n\\n```python\\ndata_soni = SoniSeries(spectrum_A)\\ndata_soni_preview = data_soni.preview_object\\ndata_soni_preview.sonify_preview()\\ndata_soni_preview.play_preview()\\n```\\n\\n\\n```python\\ndata_soni = SoniSeries(spectrum_F)\\ndata_soni_preview = data_soni.preview_object\\ndata_soni_preview.sonify_preview()\\ndata_soni_preview.play_preview()\\n```\\n\\n\\n```python\\ndata_soni = SoniSeries(spectrum_G, flatten=True)\\ndata_soni_preview = data_soni.preview_object\\ndata_soni_preview.sonify_preview()\\ndata_soni_preview.play_preview()\\n```\\n\\n\\n```python\\ndata_soni = SoniSeries(spectrum_K)\\ndata_soni_preview = data_soni.preview_object\\ndata_soni_preview.sonify_preview()\\ndata_soni_preview.play_preview()\\n```\\n\\n\\n```python\\ndata_soni = SoniSeries(spectrum_M, flatten=True)\\ndata_soni_preview = data_soni.preview_object\\ndata_soni_preview.sonify_preview()\\ndata_soni_preview.play_preview()\\n```\\n\\n\\n    ---------------------------------------------------------------------------\\n\\n    NameError                                 Traceback (most recent call last)\\n\\n    Cell In [10], line 1\\n    ----> 1 data_soni = SoniSeries(spectrum_M, flatten=True)\\n          2 data_soni_preview = data_soni.preview_object\\n          3 data_soni_preview.sonify_preview()\\n\\n\\n    NameError: name \\'spectrum_M\\' is not defined\\n\\n\\n\\n```python\\n\\n```\\n\\n\\n```python\\n\\n```\\n'"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from hubcap import transform_github_url\n",
                "\n",
                "notebook_src = 'https://github.com/spacetelescope/astronify/blob/main/notebooks/OBAFGKM_demo.ipynb'\n",
                "notebook_src = transform_github_url(notebook_src, 'raw_file')\n",
                "\n",
                "nb = notebook_to_markdown(notebook_src)\n",
                "# len(json.dumps(nb['cells']))\n",
                "type(nb)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "15416894"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from test2doc.notebook_utils import clear_outputs_of_largest_output_cells, ensure_notebook_dict\n",
                "\n",
                "notebook_filepath = '/Users/thorwhalen/Dropbox/py/proj/notebooks/003 - Scrap 2025.ipynb'\n",
                "\n",
                "import json\n",
                "nb = ensure_notebook_dict(notebook_filepath)\n",
                "len(json.dumps(nb['cells']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "len(json.dumps(nb['cells']))=15415262\n",
                        "len(json.dumps(nb['cells']))=372927\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "nb = ensure_notebook_dict(notebook_filepath)\n",
                "n = 5\n",
                "print(f\"{len(json.dumps(nb['cells']))=}\")\n",
                "\n",
                "import pathlib\n",
                "import json\n",
                "from dol import Pipe\n",
                "\n",
                "target_filepath = '/Users/thorwhalen/Dropbox/py/proj/notebooks/_003 - Scrap 2025.ipynb'\n",
                "egress = Pipe(\n",
                "    json.dumps,\n",
                "    pathlib.Path(target_filepath).write_text\n",
                ")\n",
                "clear_outputs_of_largest_output_cells(nb, n, egress=egress)\n",
                "print(f\"{len(json.dumps(nb['cells']))=}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import json\n",
                "\n",
                "nb = ensure_notebook_dict(notebook_filepath)\n",
                "n = 5\n",
                "print(f\"{len(json.dumps(nb['cells']))=}\")\n",
                "\n",
                "sorted_cells = sort_notebook_cells(\n",
                "    nb,\n",
                "    key=get_output_size,\n",
                "    reverse=True,\n",
                "    cell_egress=lambda x: x\n",
                ")\n",
                "\n",
                "def empty_output(cell):\n",
                "    if 'outputs' in cell:\n",
                "        cell['outputs'] = []\n",
                "    return cell\n",
                "\n",
                "for i in range(n):\n",
                "    c = next(sorted_cells)\n",
                "    empty_output(c)\n",
                "\n",
                "print(f\"{len(json.dumps(nb['cells']))=}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "375341"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "import pathlib \n",
                "\n",
                "target_notebook_filepath = '/Users/thorwhalen/Dropbox/py/proj/notebooks/_003 - Scrap 2025.ipynb'\n",
                "pathlib.Path(target_notebook_filepath).write_text(json.dumps(nb))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'cell_type': 'code',\n",
                            " 'execution_count': 7,\n",
                            " 'id': '89a92820',\n",
                            " 'metadata': {},\n",
                            " 'outputs': [],\n",
                            " 'source': ['import pandas as pd \\n', '\\n', \"pd.DataFrame(r['results'])\"]}"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "c"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "4"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "c = ensure_notebook_dict('/Users/thorwhalen/Dropbox/py/proj/notebooks/003 - Scrap 2025.ipynb')\n",
                "len(c)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'cell_type': 'code',\n",
                            " 'execution_count': 1,\n",
                            " 'id': '75a6a00a',\n",
                            " 'metadata': {},\n",
                            " 'outputs': [{'name': 'stderr',\n",
                            "   'output_type': 'stream',\n",
                            "   'text': ['/Users/thorwhalen/Dropbox/py/proj/i/dols/unbox/unbox/base.py:186: UserWarning: Not a version that is validated by this code: 3.10. Yielding nothing\\n',\n",
                            "    '  warnings.warn(\\n',\n",
                            "    \"/Users/thorwhalen/Dropbox/py/proj/i/dols/unbox/unbox/base.py:191: UserWarning: It seems I can't access the python builtin names data, so I'll yield nothing. Error: FileNotFoundError(2, 'No such file or directory')\\n\",\n",
                            "    '  warnings.warn(\\n']}],\n",
                            " 'source': ['import qo']}"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "len(c['cells'])\n",
                "c['cells'][0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# WIP: Converting everything to markdown"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ImportError",
                    "evalue": "failed to find libmagic.  Check your installation",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbinascii\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmimetypes\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmagic\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Callable, Dict, Any, Union\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/magic/__init__.py:209\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m m\u001b[38;5;241m.\u001b[39mfrom_descriptor(fd)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m loader\n\u001b[0;32m--> 209\u001b[0m libmagic \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m magic_t \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_void_p\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merrorcheck_null\u001b[39m(result, func, args):\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/magic/loader.py:49\u001b[0m, in \u001b[0;36mload_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m   \u001b[38;5;66;03m# It is better to raise an ImportError since we are importing magic module\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to find libmagic.  Check your installation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
                        "\u001b[0;31mImportError\u001b[0m: failed to find libmagic.  Check your installation"
                    ]
                }
            ],
            "source": [
                "\"\"\"\n",
                "Functions for detecting content types and converting various formats to markdown.\n",
                "\"\"\"\n",
                "\n",
                "import io\n",
                "import binascii\n",
                "import mimetypes\n",
                "import magic\n",
                "from typing import Optional, Callable, Dict, Any, Union\n",
                "from pathlib import Path\n",
                "from functools import partial\n",
                "\n",
                "\n",
                "def _is_pdf(data: bytes) -> bool:\n",
                "    \"\"\"Check if bytes represent a PDF file by looking for the PDF header signature.\"\"\"\n",
                "    return data[:5] == b\"%PDF-\"\n",
                "\n",
                "\n",
                "def _is_docx(data: bytes) -> bool:\n",
                "    \"\"\"Check if bytes represent a DOCX file by looking for the ZIP signature and checking if it contains word/document.xml.\"\"\"\n",
                "    if data[:4] != b\"PK\\x03\\x04\":  # ZIP file signature\n",
                "        return False\n",
                "    \n",
                "    try:\n",
                "        with io.BytesIO(data) as f:\n",
                "            import zipfile\n",
                "            with zipfile.ZipFile(f) as zip_ref:\n",
                "                return any(\"word/document.xml\" in name for name in zip_ref.namelist())\n",
                "    except zipfile.BadZipFile:\n",
                "        return False\n",
                "\n",
                "\n",
                "def _is_xlsx(data: bytes) -> bool:\n",
                "    \"\"\"Check if bytes represent an XLSX file by looking for the ZIP signature and checking if it contains xl/workbook.xml.\"\"\"\n",
                "    if data[:4] != b\"PK\\x03\\x04\":  # ZIP file signature\n",
                "        return False\n",
                "    \n",
                "    try:\n",
                "        with io.BytesIO(data) as f:\n",
                "            import zipfile\n",
                "            with zipfile.ZipFile(f) as zip_ref:\n",
                "                return any(\"xl/workbook.xml\" in name for name in zip_ref.namelist())\n",
                "    except zipfile.BadZipFile:\n",
                "        return False\n",
                "\n",
                "\n",
                "def _is_pptx(data: bytes) -> bool:\n",
                "    \"\"\"Check if bytes represent a PPTX file by looking for the ZIP signature and checking if it contains ppt/presentation.xml.\"\"\"\n",
                "    if data[:4] != b\"PK\\x03\\x04\":  # ZIP file signature\n",
                "        return False\n",
                "    \n",
                "    try:\n",
                "        with io.BytesIO(data) as f:\n",
                "            import zipfile\n",
                "            with zipfile.ZipFile(f) as zip_ref:\n",
                "                return any(\"ppt/presentation.xml\" in name for name in zip_ref.namelist())\n",
                "    except zipfile.BadZipFile:\n",
                "        return False\n",
                "\n",
                "\n",
                "def _is_html(data: bytes) -> bool:\n",
                "    \"\"\"Check if bytes likely represent an HTML file.\"\"\"\n",
                "    try:\n",
                "        text = data.decode('utf-8', errors='ignore').lower()\n",
                "        return (text.strip().startswith('<!doctype html') or \n",
                "                text.strip().startswith('<html') or \n",
                "                '<html' in text[:1000])\n",
                "    except Exception:\n",
                "        return False\n",
                "\n",
                "\n",
                "def _is_ipynb(data: bytes) -> bool:\n",
                "    \"\"\"Check if bytes represent a Jupyter notebook by checking for JSON with notebook structure.\"\"\"\n",
                "    try:\n",
                "        import json\n",
                "        notebook = json.loads(data)\n",
                "        return (\"cells\" in notebook and \n",
                "                \"metadata\" in notebook and \n",
                "                \"nbformat\" in notebook)\n",
                "    except Exception:\n",
                "        return False\n",
                "\n",
                "\n",
                "def _detect_content_type(data: bytes, key: Optional[str] = None) -> str:\n",
                "    \"\"\"\n",
                "    Detect the content type of the given bytes.\n",
                "    \n",
                "    Args:\n",
                "        data: The byte data to analyze\n",
                "        key: Optional filename that can be used for extension-based detection\n",
                "        \n",
                "    Returns:\n",
                "        String representing the detected format (e.g., 'pdf', 'docx', etc.)\n",
                "    \"\"\"\n",
                "    # Try to detect from file signature first\n",
                "    if _is_pdf(data):\n",
                "        return 'pdf'\n",
                "    elif _is_docx(data):\n",
                "        return 'docx'\n",
                "    elif _is_xlsx(data):\n",
                "        return 'xlsx'\n",
                "    elif _is_pptx(data):\n",
                "        return 'pptx'\n",
                "    elif _is_ipynb(data):\n",
                "        return 'ipynb'\n",
                "    elif _is_html(data):\n",
                "        return 'html'\n",
                "    \n",
                "    # If key is provided, try to get extension\n",
                "    if key:\n",
                "        extension = Path(key).suffix.lstrip('.').lower()\n",
                "        if extension in ('pdf', 'docx', 'doc', 'xlsx', 'xls', 'pptx', 'ppt', 'html', 'ipynb'):\n",
                "            return extension\n",
                "    \n",
                "    # Try to detect with python-magic if available\n",
                "    try:\n",
                "        mime = magic.from_buffer(data, mime=True)\n",
                "        if mime == 'application/pdf':\n",
                "            return 'pdf'\n",
                "        elif mime in ('application/vnd.openxmlformats-officedocument.wordprocessingml.document', \n",
                "                      'application/msword'):\n",
                "            return 'docx'\n",
                "        elif mime in ('application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
                "                      'application/vnd.ms-excel'):\n",
                "            return 'xlsx'\n",
                "        elif mime in ('application/vnd.openxmlformats-officedocument.presentationml.presentation',\n",
                "                      'application/vnd.ms-powerpoint'):\n",
                "            return 'pptx'\n",
                "        elif mime in ('text/html', 'application/xhtml+xml'):\n",
                "            return 'html'\n",
                "        elif mime == 'application/json':\n",
                "            # Need additional check for ipynb files\n",
                "            try:\n",
                "                if _is_ipynb(data):\n",
                "                    return 'ipynb'\n",
                "            except Exception:\n",
                "                pass\n",
                "    except Exception:\n",
                "        pass\n",
                "    \n",
                "    # If we reach here, we couldn't definitively identify the format\n",
                "    return 'unknown'\n",
                "\n",
                "\n",
                "def guess_and_convert_to_markdown(\n",
                "    data: bytes,\n",
                "    key: Optional[str] = None,\n",
                "    *,\n",
                "    converters: Optional[Dict[str, Callable]] = None,\n",
                "    verbose: bool = False\n",
                ") -> Optional[str]:\n",
                "    \"\"\"\n",
                "    Attempts to identify the content type of the given bytes and convert it to markdown\n",
                "    using appropriate converters.\n",
                "    \n",
                "    Args:\n",
                "        data: The byte data to convert\n",
                "        key: Optional identifier/filename to help with content detection and for verbose output\n",
                "        converters: Optional dictionary of content type to converter function mappings\n",
                "                   If None, uses the default converters from contexts module\n",
                "        verbose: Whether to print information about the conversion process\n",
                "        \n",
                "    Returns:\n",
                "        Markdown string if conversion successful, None otherwise\n",
                "    \"\"\"\n",
                "    from contaix import bytes_to_markdown, dflt_converters\n",
                "    \n",
                "    if converters is None:\n",
                "        converters = dflt_converters\n",
                "    \n",
                "    def _log(msg):\n",
                "        if verbose:\n",
                "            key_info = f\"[{key}] \" if key else \"\"\n",
                "            print(f\"{key_info}{msg}\")\n",
                "    \n",
                "    # First try to detect the content type\n",
                "    content_type = _detect_content_type(data, key)\n",
                "    \n",
                "    if content_type == 'unknown':\n",
                "        _log(\"Could not detect content type. Trying common formats...\")\n",
                "        \n",
                "        # Try the most common formats in a reasonable order\n",
                "        formats_to_try = ['pdf', 'docx', 'html', 'xlsx', 'ipynb', 'pptx']\n",
                "        \n",
                "        for fmt in formats_to_try:\n",
                "            if fmt in converters:\n",
                "                _log(f\"Attempting conversion as {fmt}...\")\n",
                "                try:\n",
                "                    result = bytes_to_markdown(data, fmt, converters=converters)\n",
                "                    _log(f\"Successfully converted content as {fmt}\")\n",
                "                    return result\n",
                "                except Exception as e:\n",
                "                    _log(f\"Failed to convert as {fmt}: {str(e)}\")\n",
                "        \n",
                "        _log(\"Could not convert content with any available converter\")\n",
                "        return None\n",
                "    \n",
                "    # Content type detected, try the corresponding converter\n",
                "    _log(f\"Detected content type: {content_type}\")\n",
                "    \n",
                "    if content_type in converters:\n",
                "        try:\n",
                "            result = bytes_to_markdown(data, content_type, converters=converters)\n",
                "            _log(f\"Successfully converted {content_type} to markdown\")\n",
                "            return result\n",
                "        except Exception as e:\n",
                "            _log(f\"Failed to convert {content_type} to markdown: {str(e)}\")\n",
                "            return None\n",
                "    else:\n",
                "        _log(f\"No converter available for {content_type}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "Functions for detecting content types and converting various formats to markdown.\n",
                "\"\"\"\n",
                "\n",
                "import io\n",
                "import binascii\n",
                "import mimetypes\n",
                "from typing import Optional, Callable, Dict, Any, Union\n",
                "from pathlib import Path\n",
                "from functools import partial\n",
                "\n",
                "\n",
                "def _is_pdf(data: bytes) -> bool:\n",
                "    \"\"\"Check if bytes represent a PDF file by looking for the PDF header signature.\"\"\"\n",
                "    return data[:5] == b\"%PDF-\"\n",
                "\n",
                "\n",
                "def _is_docx(data: bytes) -> bool:\n",
                "    \"\"\"Check if bytes represent a DOCX file by looking for the ZIP signature and checking if it contains word/document.xml.\"\"\"\n",
                "    if data[:4] != b\"PK\\x03\\x04\":  # ZIP file signature\n",
                "        return False\n",
                "    \n",
                "    try:\n",
                "        with io.BytesIO(data) as f:\n",
                "            import zipfile\n",
                "            with zipfile.ZipFile(f) as zip_ref:\n",
                "                return any(\"word/document.xml\" in name for name in zip_ref.namelist())\n",
                "    except zipfile.BadZipFile:\n",
                "        return False\n",
                "\n",
                "\n",
                "def _is_xlsx(data: bytes) -> bool:\n",
                "    \"\"\"Check if bytes represent an XLSX file by looking for the ZIP signature and checking if it contains xl/workbook.xml.\"\"\"\n",
                "    if data[:4] != b\"PK\\x03\\x04\":  # ZIP file signature\n",
                "        return False\n",
                "    \n",
                "    try:\n",
                "        with io.BytesIO(data) as f:\n",
                "            import zipfile\n",
                "            with zipfile.ZipFile(f) as zip_ref:\n",
                "                return any(\"xl/workbook.xml\" in name for name in zip_ref.namelist())\n",
                "    except zipfile.BadZipFile:\n",
                "        return False\n",
                "\n",
                "\n",
                "def _is_pptx(data: bytes) -> bool:\n",
                "    \"\"\"Check if bytes represent a PPTX file by looking for the ZIP signature and checking if it contains ppt/presentation.xml.\"\"\"\n",
                "    if data[:4] != b\"PK\\x03\\x04\":  # ZIP file signature\n",
                "        return False\n",
                "    \n",
                "    try:\n",
                "        with io.BytesIO(data) as f:\n",
                "            import zipfile\n",
                "            with zipfile.ZipFile(f) as zip_ref:\n",
                "                return any(\"ppt/presentation.xml\" in name for name in zip_ref.namelist())\n",
                "    except zipfile.BadZipFile:\n",
                "        return False\n",
                "\n",
                "\n",
                "def _is_html(data: bytes) -> bool:\n",
                "    \"\"\"Check if bytes likely represent an HTML file.\"\"\"\n",
                "    try:\n",
                "        text = data.decode('utf-8', errors='ignore').lower()\n",
                "        return (text.strip().startswith('<!doctype html') or \n",
                "                text.strip().startswith('<html') or \n",
                "                '<html' in text[:1000])\n",
                "    except Exception:\n",
                "        return False\n",
                "\n",
                "\n",
                "def _is_ipynb(data: bytes) -> bool:\n",
                "    \"\"\"Check if bytes represent a Jupyter notebook by checking for JSON with notebook structure.\"\"\"\n",
                "    try:\n",
                "        import json\n",
                "        notebook = json.loads(data)\n",
                "        return (\"cells\" in notebook and \n",
                "                \"metadata\" in notebook and \n",
                "                \"nbformat\" in notebook)\n",
                "    except Exception:\n",
                "        return False\n",
                "\n",
                "\n",
                "def _detect_content_type(data: bytes, key: Optional[str] = None) -> str:\n",
                "    \"\"\"\n",
                "    Detect the content type of the given bytes.\n",
                "    \n",
                "    Args:\n",
                "        data: The byte data to analyze\n",
                "        key: Optional filename that can be used for extension-based detection\n",
                "        \n",
                "    Returns:\n",
                "        String representing the detected format (e.g., 'pdf', 'docx', etc.)\n",
                "    \"\"\"\n",
                "    # Try to detect from file signature first\n",
                "    if _is_pdf(data):\n",
                "        return 'pdf'\n",
                "    elif _is_docx(data):\n",
                "        return 'docx'\n",
                "    elif _is_xlsx(data):\n",
                "        return 'xlsx'\n",
                "    elif _is_pptx(data):\n",
                "        return 'pptx'\n",
                "    elif _is_ipynb(data):\n",
                "        return 'ipynb'\n",
                "    elif _is_html(data):\n",
                "        return 'html'\n",
                "    \n",
                "    # If key is provided, try to get extension\n",
                "    if key:\n",
                "        extension = Path(key).suffix.lstrip('.').lower()\n",
                "        if extension in ('pdf', 'docx', 'doc', 'xlsx', 'xls', 'pptx', 'ppt', 'html', 'ipynb'):\n",
                "            return extension\n",
                "    \n",
                "    # Try to use mimetypes for extension-based detection if we have a key\n",
                "    if key:\n",
                "        mime_type, _ = mimetypes.guess_type(key)\n",
                "        if mime_type:\n",
                "            if mime_type == 'application/pdf':\n",
                "                return 'pdf'\n",
                "            elif mime_type in ('application/vnd.openxmlformats-officedocument.wordprocessingml.document', \n",
                "                       'application/msword'):\n",
                "                return 'docx'\n",
                "            elif mime_type in ('application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
                "                       'application/vnd.ms-excel'):\n",
                "                return 'xlsx'\n",
                "            elif mime_type in ('application/vnd.openxmlformats-officedocument.presentationml.presentation',\n",
                "                       'application/vnd.ms-powerpoint'):\n",
                "                return 'pptx'\n",
                "            elif mime_type in ('text/html', 'application/xhtml+xml'):\n",
                "                return 'html'\n",
                "            elif mime_type == 'application/json':\n",
                "                # Need additional check for ipynb files\n",
                "                try:\n",
                "                    if _is_ipynb(data):\n",
                "                        return 'ipynb'\n",
                "                except Exception:\n",
                "                    pass\n",
                "    \n",
                "    # Additional basic content checks based on text patterns\n",
                "    try:\n",
                "        # Try to decode first ~1000 bytes to see if it's text\n",
                "        sample = data[:1000].decode('utf-8', errors='ignore')\n",
                "        \n",
                "        # Check for JSON\n",
                "        if sample.strip().startswith('{') and sample.strip().endswith('}'):\n",
                "            try:\n",
                "                if _is_ipynb(data):\n",
                "                    return 'ipynb'\n",
                "            except Exception:\n",
                "                pass\n",
                "    except Exception:\n",
                "        pass\n",
                "    \n",
                "    # If we reach here, we couldn't definitively identify the format\n",
                "    return 'unknown'\n",
                "\n",
                "\n",
                "def try_to_convert_to_markdown(\n",
                "    data: bytes,\n",
                "    key: Optional[str] = None,\n",
                "    *,\n",
                "    converters: Optional[Dict[str, Callable]] = None,\n",
                "    verbose: bool = False\n",
                ") -> Optional[str]:\n",
                "    \"\"\"\n",
                "    Attempts to identify the content type of the given bytes and convert it to markdown\n",
                "    using appropriate converters from contaix.\n",
                "    \n",
                "    Args:\n",
                "        data: The byte data to convert\n",
                "        key: Optional identifier/filename to help with content detection and for verbose output\n",
                "        converters: Optional dictionary of content type to converter function mappings\n",
                "                   If None, uses the default converters from contexts module\n",
                "        verbose: Whether to print information about the conversion process\n",
                "        \n",
                "    Returns:\n",
                "        Markdown string if conversion successful, None otherwise\n",
                "    \"\"\"\n",
                "    from contaix import bytes_to_markdown, dflt_converters\n",
                "    \n",
                "    if converters is None:\n",
                "        converters = dflt_converters\n",
                "    \n",
                "    def _log(msg):\n",
                "        if verbose:\n",
                "            key_info = f\"[{key}] \" if key else \"\"\n",
                "            print(f\"{key_info}{msg}\")\n",
                "    \n",
                "    # First try to detect the content type\n",
                "    content_type = _detect_content_type(data, key)\n",
                "    \n",
                "    if content_type == 'unknown':\n",
                "        _log(\"Could not detect content type. Trying common formats...\")\n",
                "        \n",
                "        # Try the most common formats in a reasonable order\n",
                "        formats_to_try = ['pdf', 'docx', 'html', 'xlsx', 'ipynb', 'pptx']\n",
                "        \n",
                "        for fmt in formats_to_try:\n",
                "            if fmt in converters:\n",
                "                _log(f\"Attempting conversion as {fmt}...\")\n",
                "                try:\n",
                "                    result = bytes_to_markdown(data, fmt, converters=converters)\n",
                "                    _log(f\"Successfully converted content as {fmt}\")\n",
                "                    return result\n",
                "                except Exception as e:\n",
                "                    _log(f\"Failed to convert as {fmt}: {str(e)}\")\n",
                "        \n",
                "        _log(\"Could not convert content with any available converter\")\n",
                "        return None\n",
                "    \n",
                "    # Content type detected, try the corresponding converter\n",
                "    _log(f\"Detected content type: {content_type}\")\n",
                "    \n",
                "    if content_type in converters:\n",
                "        try:\n",
                "            result = bytes_to_markdown(data, content_type, converters=converters)\n",
                "            _log(f\"Successfully converted {content_type} to markdown\")\n",
                "            return result\n",
                "        except Exception as e:\n",
                "            _log(f\"Failed to convert {content_type} to markdown: {str(e)}\")\n",
                "            return None\n",
                "    else:\n",
                "        _log(f\"No converter available for {content_type}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[https://webaudio.github.io/web-audio-api] Detected content type: html\n",
                        "[https://webaudio.github.io/web-audio-api] Successfully converted html to markdown\n",
                        "[https://www.unoosa.org/documents/pdf/Space4PersonswithDisabilites/UNOOSA_Special_Report_on_Sonification_2023.pdf] Detected content type: pdf\n",
                        "[https://www.unoosa.org/documents/pdf/Space4PersonswithDisabilites/UNOOSA_Special_Report_on_Sonification_2023.pdf] Successfully converted pdf to markdown\n",
                        "[https://www.loudnumbers.net] Detected content type: html\n",
                        "[https://www.loudnumbers.net] Successfully converted html to markdown\n",
                        "[https://github.com/thorwhalen/sonification/discussions/4#discussioncomment-12614565] Detected content type: html\n",
                        "[https://github.com/thorwhalen/sonification/discussions/4#discussioncomment-12614565] Successfully converted html to markdown\n",
                        "[https://github.com/thorwhalen/sonification/discussions/4#discussioncomment-12625392] Detected content type: html\n",
                        "[https://github.com/thorwhalen/sonification/discussions/4#discussioncomment-12625392] Successfully converted html to markdown\n",
                        "[https://github.com/thorwhalen/sonification/discussions/4#discussioncomment-12614294] Detected content type: html\n",
                        "[https://github.com/thorwhalen/sonification/discussions/4#discussioncomment-12614294] Successfully converted html to markdown\n",
                        "[https://github.com/thorwhalen/sonification/discussions/4#discussioncomment-12616646] Detected content type: html\n",
                        "[https://github.com/thorwhalen/sonification/discussions/4#discussioncomment-12616646] Successfully converted html to markdown\n",
                        "[https://www.academia.edu/68785319/The_Importance_of_Interaction_in_Sonification] Detected content type: html\n",
                        "[https://www.academia.edu/68785319/The_Importance_of_Interaction_in_Sonification] Successfully converted html to markdown\n",
                        "[https://www.academia.edu/78589380/HCI_Design_and_Interactive_Sonification_for_Fingers_and_Ears] Detected content type: html\n",
                        "[https://www.academia.edu/78589380/HCI_Design_and_Interactive_Sonification_for_Fingers_and_Ears] Successfully converted html to markdown\n",
                        "[https://www.academia.edu/Documents/in/Data_Sonification] Detected content type: html\n",
                        "[https://www.academia.edu/Documents/in/Data_Sonification] Successfully converted html to markdown\n",
                        "[https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04362-7] Detected content type: html\n",
                        "[https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04362-7] Successfully converted html to markdown\n",
                        "[https://programminghistorian.org/en/lessons/sonification] Detected content type: html\n",
                        "[https://programminghistorian.org/en/lessons/sonification] Successfully converted html to markdown\n",
                        "[https://www.mcs.anl.gov/~kaper/Sonification/index.html] Detected content type: html\n",
                        "[https://www.mcs.anl.gov/~kaper/Sonification/index.html] Successfully converted html to markdown\n",
                        "[https://opensonifications.net/open-sonifications-DIS.pdf] Detected content type: pdf\n",
                        "[https://opensonifications.net/open-sonifications-DIS.pdf] Successfully converted pdf to markdown\n",
                        "[https://sonification.design] Detected content type: html\n",
                        "[https://sonification.design] Successfully converted html to markdown\n",
                        "[https://arxiv.org/abs/2209.04465] Detected content type: html\n",
                        "[https://arxiv.org/abs/2209.04465] Successfully converted html to markdown\n",
                        "[https://arxiv.org/abs/2208.02494] Detected content type: html\n",
                        "[https://arxiv.org/abs/2208.02494] Successfully converted html to markdown\n",
                        "[https://arxiv.org/abs/2108.09537] Detected content type: html\n",
                        "[https://arxiv.org/abs/2108.09537] Successfully converted html to markdown\n",
                        "[https://arxiv.org/abs/2002.00002] Detected content type: html\n",
                        "[https://arxiv.org/abs/2002.00002] Successfully converted html to markdown\n",
                        "[https://arxiv.org/abs/2402.00156] Detected content type: html\n",
                        "[https://arxiv.org/abs/2402.00156] Successfully converted html to markdown\n",
                        "[https://arxiv.org/abs/2001.00001] Detected content type: html\n",
                        "[https://arxiv.org/abs/2001.00001] Successfully converted html to markdown\n",
                        "[https://arxiv.org/abs/2404.00016] Detected content type: html\n",
                        "[https://arxiv.org/abs/2404.00016] Successfully converted html to markdown\n",
                        "[https://arxiv.org/abs/2412.09152] Detected content type: html\n",
                        "[https://arxiv.org/abs/2412.09152] Successfully converted html to markdown\n",
                        "[https://openreview.net/forum?id=B1x1ma4tDr] Detected content type: html\n",
                        "[https://openreview.net/forum?id=B1x1ma4tDr] Successfully converted html to markdown\n",
                        "[https://www.mdpi.com/journal/algorithms/special_issues/MU_MS] Detected content type: html\n",
                        "[https://www.mdpi.com/journal/algorithms/special_issues/MU_MS] Successfully converted html to markdown\n",
                        "[https://vbn.aau.dk/files/466610617/SMC_2021_paper_58.pdf] Detected content type: pdf\n",
                        "[https://vbn.aau.dk/files/466610617/SMC_2021_paper_58.pdf] Successfully converted pdf to markdown\n",
                        "[https://doi.org/10.3389/fdata.2023.1206081] Detected content type: html\n",
                        "[https://doi.org/10.3389/fdata.2023.1206081] Successfully converted html to markdown\n",
                        "[https://doi.org/10.3389/fcomm.2020.00046] Detected content type: html\n",
                        "[https://doi.org/10.3389/fcomm.2020.00046] Successfully converted html to markdown\n",
                        "[https://doi.org/10.3389/fpsyg.2022.1020102] Detected content type: html\n",
                        "[https://doi.org/10.3389/fpsyg.2022.1020102] Successfully converted html to markdown\n",
                        "[https://doi.org/10.3389/fcomm.2024.1288896] Detected content type: html\n",
                        "[https://doi.org/10.3389/fcomm.2024.1288896] Successfully converted html to markdown\n",
                        "[https://doi.org/10.3390/s24010065] Detected content type: html\n",
                        "[https://doi.org/10.3390/s24010065] Successfully converted html to markdown\n",
                        "[https://hdl.handle.net/1853/49960] Detected content type: html\n",
                        "[https://hdl.handle.net/1853/49960] Successfully converted html to markdown\n",
                        "[https://hdl.handle.net/1853/66336] Detected content type: html\n",
                        "[https://hdl.handle.net/1853/66336] Successfully converted html to markdown\n",
                        "[https://www.icad.org] Detected content type: html\n",
                        "[https://www.icad.org] Successfully converted html to markdown\n",
                        "[https://link.springer.com/chapter/10.1007/978-3-540-70816-2_18] Detected content type: html\n",
                        "[https://link.springer.com/chapter/10.1007/978-3-540-70816-2_18] Successfully converted html to markdown\n",
                        "[http://www.fon.hum.uva.nl/praat] Detected content type: html\n",
                        "[http://www.fon.hum.uva.nl/praat] Successfully converted html to markdown\n"
                    ]
                }
            ],
            "source": [
                "from graze import Graze\n",
                "\n",
                "g = Graze('/Users/thorwhalen/Dropbox/_odata/ai_contexts/projects/sonification/sonification_files')\n",
                "\n",
                "def markdown_snippets(store):\n",
                "    for k, v in store.items():\n",
                "        md = try_to_convert_to_markdown(v, k, verbose=True)\n",
                "        yield k, md\n",
                "\n",
                "d = dict(markdown_snippets(g))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['https://webaudio.github.io/web-audio-api',\n",
                            " 'https://www.unoosa.org/documents/pdf/Space4PersonswithDisabilites/UNOOSA_Special_Report_on_Sonification_2023.pdf',\n",
                            " 'https://www.loudnumbers.net',\n",
                            " 'https://github.com/thorwhalen/sonification/discussions/4#discussioncomment-12614565',\n",
                            " 'https://github.com/thorwhalen/sonification/discussions/4#discussioncomment-12625392',\n",
                            " 'https://github.com/thorwhalen/sonification/discussions/4#discussioncomment-12614294',\n",
                            " 'https://github.com/thorwhalen/sonification/discussions/4#discussioncomment-12616646',\n",
                            " 'https://www.academia.edu/68785319/The_Importance_of_Interaction_in_Sonification',\n",
                            " 'https://www.academia.edu/78589380/HCI_Design_and_Interactive_Sonification_for_Fingers_and_Ears',\n",
                            " 'https://www.academia.edu/Documents/in/Data_Sonification',\n",
                            " 'https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04362-7',\n",
                            " 'https://programminghistorian.org/en/lessons/sonification',\n",
                            " 'https://www.mcs.anl.gov/~kaper/Sonification/index.html',\n",
                            " 'https://opensonifications.net/open-sonifications-DIS.pdf',\n",
                            " 'https://sonification.design',\n",
                            " 'https://arxiv.org/abs/2209.04465',\n",
                            " 'https://arxiv.org/abs/2208.02494',\n",
                            " 'https://arxiv.org/abs/2108.09537',\n",
                            " 'https://arxiv.org/abs/2002.00002',\n",
                            " 'https://arxiv.org/abs/2402.00156',\n",
                            " 'https://arxiv.org/abs/2001.00001',\n",
                            " 'https://arxiv.org/abs/2404.00016',\n",
                            " 'https://arxiv.org/abs/2412.09152',\n",
                            " 'https://openreview.net/forum?id=B1x1ma4tDr',\n",
                            " 'https://www.mdpi.com/journal/algorithms/special_issues/MU_MS',\n",
                            " 'https://vbn.aau.dk/files/466610617/SMC_2021_paper_58.pdf',\n",
                            " 'https://doi.org/10.3389/fdata.2023.1206081',\n",
                            " 'https://doi.org/10.3389/fcomm.2020.00046',\n",
                            " 'https://doi.org/10.3389/fpsyg.2022.1020102',\n",
                            " 'https://doi.org/10.3389/fcomm.2024.1288896',\n",
                            " 'https://doi.org/10.3390/s24010065',\n",
                            " 'https://hdl.handle.net/1853/49960',\n",
                            " 'https://hdl.handle.net/1853/66336',\n",
                            " 'https://www.icad.org',\n",
                            " 'https://link.springer.com/chapter/10.1007/978-3-540-70816-2_18',\n",
                            " 'http://www.fon.hum.uva.nl/praat']"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "list(d)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Scrap"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from contaix import bytes_store_to_markdown_store\n",
                "from contaix import extensions_not_supported_by_converters\n",
                "\n",
                "src_dir = 'ENTER_YOUR_SOURCE_DIR_HERE'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# If you want to \n",
                "targ_dir = None  # also enter your target folder here\n",
                "\n",
                "if targ_dir is not None:\n",
                "    assert not extensions_not_supported_by_converters(src_dir), (\n",
                "        \"some extensions are not supported by the converters\"\n",
                "    )\n",
                "\n",
                "    target_store = bytes_store_to_markdown_store(src_dir, targ_dir)\n",
                "\n",
                "    print(f\"You now have a folder with {len(target_store)} markdown files in it: {targ_dir}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "50"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# doing it in memory\n",
                "in_memory_target_store = bytes_store_to_markdown_store(src_dir, target_store={})\n",
                "len(in_memory_target_store)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "14450386"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from contaix import aggregate_store\n",
                "\n",
                "md_string = aggregate_store(in_memory_target_store)\n",
                "len(md_string)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'docx.md', 'pdf.md', 'pptx.md', 'xlsx.md'}"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from contaix import get_extension\n",
                "\n",
                "extensions = set(map(lambda x: '.'.join(x.split('.')[-2:]), in_memory_target_store))\n",
                "extensions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "aggregate for extension pptx.md has 72835 characters\n",
                        "aggregate for extension xlsx.md has 1981094 characters\n",
                        "aggregate for extension pdf.md has 1577353 characters\n",
                        "aggregate for extension docx.md has 10819098 characters\n"
                    ]
                }
            ],
            "source": [
                "aggregate_stores_by_ext = {}\n",
                "\n",
                "for ext in extensions:\n",
                "    substore = {k: v for k, v in in_memory_target_store.items() if k.endswith(ext)}\n",
                "    aggregate_key = \"aggregate.{}\".format(ext)\n",
                "    aggregate_stores_by_ext[aggregate_key] = aggregate_store(substore)\n",
                "    print(f\"aggregate for extension {ext} has {len(aggregate_stores_by_ext[aggregate_key])} characters\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "# if you want to save this to disk\n",
                "import dol \n",
                "\n",
                "target_sub_stores_folders = '~'  # change here\n",
                "aggregate_stores_by_ext_files = dol.TextFiles(target_sub_stores_folders)\n",
                "\n",
                "aggregate_stores_by_ext_files.update(aggregate_stores_by_ext)\n",
                "# and now you have some files saved to target_sub_stores_folders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "p10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}